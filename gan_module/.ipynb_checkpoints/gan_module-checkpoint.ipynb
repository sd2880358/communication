{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from IPython import display\n",
    "import mlcTest as mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import os,sys\n",
    "import scipy.io as sc\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import math\n",
    "def dataset(dataFile, labelFile):\n",
    "    dataFile = \"../ML_Symbol_Gen-main/\" + dataFile\n",
    "    labelFile = \"../ML_Symbol_Gen-main/\" + labelFile\n",
    "    my_data = sc.loadmat(dataFile)\n",
    "    my_labels = sc.loadmat(labelFile)\n",
    "    my_data = my_data['Y']\n",
    "    X = my_labels['L_S_x']\n",
    "    myOrig = table_data(my_data, my_labels['L_Constellations'][0], X)\n",
    "    mytable = assign_label(myOrig)\n",
    "    return mytable\n",
    "\n",
    "\n",
    "def assign_label(data):\n",
    "    c_4 = [1,-1]\n",
    "    c_16 = [3,1,-1,-3]\n",
    "    c_16r = [-3,-1,1,3]\n",
    "    cons_4 = np.dot(np.sqrt(0.5),[complex(i,j)for i in c_4 for j in c_4])\n",
    "    cons_16 = np.array([complex(i,j)for j in c_16 for i in c_16r])\n",
    "    cons_16 = cons_16/np.sqrt(np.mean(np.abs(cons_16)**2))\n",
    "    cons4 = data[data.cons==1]\n",
    "    cons4_label = np.array([[cons_4[i-1]]for i in cons4.label])\n",
    "    cons16 = data[data.cons==2]\n",
    "    cons16_label = np.array([[cons_16[i-1]]for i in cons16.label.to_numpy().real.astype(int)])\n",
    "    data[data.cons==2].index\n",
    "    data['buffer'] = 0\n",
    "    data['buffer'] = 0\n",
    "    data.iloc[data[data.cons==1].index, 5] = cons4_label\n",
    "    data.iloc[data[data.cons==2].index, 5] = cons16_label\n",
    "    data['label_real'] = data.buffer.to_numpy().real\n",
    "    data['label_imag'] = data.buffer.to_numpy().imag\n",
    "    myTest = data.copy()\n",
    "    myTest.loc[myTest.cons == 2, 'label'] = myTest.loc[myTest.cons == 2, 'label'] + 4\n",
    "    myTest.label = myTest.label - 1\n",
    "    return myTest\n",
    "\n",
    "\n",
    "def table_data(my_data, cons, label):\n",
    "    block = my_data.shape[1]\n",
    "    my_data_size = my_data.shape[0] * block\n",
    "    my_data_div = my_data.T.reshape(my_data_size, )\n",
    "    cons_array = np.array([[cons[i]] * my_data.shape[0] for i in range(0, block)]).reshape(my_data_size, )\n",
    "    block_array = np.array([([i + 1] * my_data.shape[0]) for i in range(0, block)]).reshape(my_data_size, )\n",
    "    label_array = label.T.reshape(my_data_size, )\n",
    "    test_pd = pd.DataFrame({'real': my_data_div.real, 'imag': my_data_div.imag,\n",
    "                            'cons': cons_array, 'block': block_array,\n",
    "                            'label': label_array})\n",
    "    return test_pd\n",
    "\n",
    "\n",
    "def make_generator():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(32, (1, 1), activation='relu', input_shape=(1, 50, 2)))\n",
    "    model.add(layers.MaxPooling2D((1, 1)))\n",
    "    model.add(layers.Conv2D(64, (1, 1), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((1, 1)))\n",
    "    model.add(layers.Conv2D(64, (1, 1), activation='relu'))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(2))\n",
    "    return model\n",
    "\n",
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Reshape((50, 2, 1)))\n",
    "    model.add(layers.Conv2D(64, (2, 1), strides=(1, 1), padding='same',\n",
    "                                     input_shape=[1, 50, 2]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "    return model\n",
    "\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "def identity_loss(real, fake):\n",
    "    loss = tf.reduce_mean(tf.abs(real - fake))\n",
    "    return LAMBDA * 0.5 * loss\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(total, label):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        s = generator_s(total, training=True)\n",
    "        n = generator_n(total, training=True)\n",
    "        i = generator_i(s, training=True)\n",
    "        gen = (s + n + i)\n",
    "        fake_t = discriminator_t(gen, training=True)\n",
    "        real_t = discriminator_t(total, training=True)\n",
    "        gen_loss = generator_loss(fake_t)\n",
    "        fake_d = discriminator_d(s, training=True)\n",
    "        real_d = discriminator_d(label, training=True)\n",
    "        gen_s_loss = generator_loss(fake_d)\n",
    "        disc_t_loss = discriminator_loss(real_t, fake_t)\n",
    "        disc_d_loss = discriminator_loss(real_d, fake_d)\n",
    "        identity_s_loss = identity_loss(label, s)\n",
    "        identity_g_loss = identity_loss(total, gen)\n",
    "        total_gen_loss = 1/2 * gen_s_loss + gen_loss\n",
    "        total_s_loss = identity_g_loss + identity_s_loss + total_gen_loss\n",
    "        total_n_loss = identity_g_loss + total_gen_loss\n",
    "        total_i_loss = identity_g_loss + total_gen_loss\n",
    "\n",
    "    gradients_of_s_generator = tape.gradient(total_s_loss, generator_s.trainable_variables)\n",
    "    gradients_of_i_generator = tape.gradient(total_i_loss, generator_i.trainable_variables)\n",
    "    gradients_of_n_generator = tape.gradient(total_n_loss, generator_n.trainable_variables)\n",
    "    gradients_of_discriminator_t = tape.gradient(disc_t_loss, discriminator_t.trainable_variables)\n",
    "    gradients_of_discriminator_d = tape.gradient(disc_d_loss, discriminator_d.trainable_variables)\n",
    "    generator_s_optimizer.apply_gradients(zip(gradients_of_s_generator, generator_s.trainable_variables))\n",
    "    generator_i_optimizer.apply_gradients(zip(gradients_of_i_generator, generator_i.trainable_variables))\n",
    "    generator_n_optimizer.apply_gradients(zip(gradients_of_n_generator, generator_n.trainable_variables))\n",
    "    discriminator_t_optimizer.apply_gradients(zip(gradients_of_discriminator_t, discriminator_t.trainable_variables))\n",
    "    discriminator_d_optimizer.apply_gradients(zip(gradients_of_discriminator_d, discriminator_d.trainable_variables))\n",
    "\n",
    "def shuffle_data(my_table):\n",
    "    real_y = (2*my_table.real.min())/(my_table.real.max() - my_table.real.min()) + 1\n",
    "    real_x = (my_table.real.max()) / (1 + real_y)\n",
    "    imag_y = (2*my_table.imag.min())/(my_table.imag.max() - my_table.imag.min()) + 1\n",
    "    imag_x = (my_table.imag.max()) / (1 + imag_y)\n",
    "    my_table.real = (my_table.real / real_x) - real_y\n",
    "    my_table.imag = (my_table.imag/ imag_x) - imag_y\n",
    "    train_feature = data.loc[:, ('real', 'imag')]\n",
    "    train_label = data.loc[:, ('label_real', 'label_imag')]\n",
    "    test_feature = tf.cast(train_feature, tf.float32)\n",
    "    test_label = tf.cast(train_label, tf.float32)\n",
    "    test_feature = tf.reshape(test_feature,(1000,1,50,2))\n",
    "    test_label = tf.reshape(test_label, (1000,1,50,2))\n",
    "    symbol = data.loc[:, 'label']\n",
    "    symbol = tf.reshape(symbol, (1000,1, 50))\n",
    "    return test_feature, test_label, symbol \n",
    "\n",
    "generator_s = make_generator()\n",
    "generator_n = make_generator()\n",
    "generator_i = make_generator()\n",
    "discriminator_t = make_discriminator_model()\n",
    "discriminator_d = make_discriminator_model()\n",
    "\n",
    "\n",
    "generator_s_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "generator_n_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "generator_i_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "discriminator_d_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_t_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "checkpoint_path = \"./checkpoints/method8\"\n",
    "ckpt = tf.train.Checkpoint(generator_s=generator_s,\n",
    "                           generator_n=generator_n,\n",
    "                           generator_i=generator_i,\n",
    "                           discriminator_t=discriminator_t,\n",
    "                           discriminator_d=discriminator_d,\n",
    "                           generator_s_optimizer=generator_s_optimizer,\n",
    "                           generator_n_optimizer=generator_n_optimizer,\n",
    "                           generator_i_optimizer=generator_i_optimizer,\n",
    "                           discriminator_d_optimizer=discriminator_d_optimizer,\n",
    "                           discriminator_t_optimizer=discriminator_t_optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')\n",
    "LAMBDA = 10\n",
    "EPOCHS = 500\n",
    "data1 = \"my_data\"\n",
    "data1_label = \"my_labels\"\n",
    "data = dataset(data1, data1_label)\n",
    "file_directory = './result/tes2/'\n",
    "f, l, s = shuffle_data(data)\n",
    "\n",
    "BUFFER_SIZE = 50\n",
    "BATCH_SIZE = 256\n",
    "train_f = tf.data.Dataset.from_tensor_slices(f).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "train_l = tf.data.Dataset.from_tensor_slices(l).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 100\n",
    "BATCH_SIZE = 50\n",
    "train_f = tf.data.Dataset.from_tensor_slices(f).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "train_l = tf.data.Dataset.from_tensor_slices(l).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clssification():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(16, (1, 1), activation='relu', input_shape=(1, 50, 2)))\n",
    "    model.add(layers.MaxPooling2D(1,1))\n",
    "    model.add(layers.Conv2D(32, 3, padding='same', activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(1,1))\n",
    "    model.add(layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(1,1))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(20))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 1, 50, 16)         48        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 1, 50, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 1, 50, 32)         4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 1, 50, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 1, 50, 64)         18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 1, 50, 64)         0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1, 50, 64)         4160      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1, 50, 20)         1300      \n",
      "=================================================================\n",
      "Total params: 28,644\n",
      "Trainable params: 28,644\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn = clssification()\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = \"my_data1\"\n",
    "test_label = \"my_labels1\"\n",
    "test_table = dataset(test_data, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_f, t_l, t_s = shuffle_data(test_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = cnn.fit(t_f, t_s, epochs=100, verbose=0,\n",
    "                    validation_data=(t_f,t_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_s = generator_s(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cnn(test_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_f = generator_s(t_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.5768998861312866, 0.43522000312805176]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.evaluate(test_f, t_s, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = clssification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "normal_his = normal.fit(f, s, epochs=100, verbose=0,\n",
    "                    validation_data=(f,s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_model = tf.keras.Sequential([cnn, tf.keras.layers.Softmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[1.90992644e-08, 2.63025641e-01, 4.55183478e-15, ...,\n",
       "          5.89987044e-14, 9.96230112e-11, 1.09929427e-11],\n",
       "         [6.73178164e-03, 3.45420470e-10, 9.42797549e-18, ...,\n",
       "          3.67741417e-19, 8.25948538e-23, 1.04197947e-23],\n",
       "         [0.00000000e+00, 0.00000000e+00, 2.96170532e-20, ...,\n",
       "          2.72257106e-09, 3.78875481e-03, 9.85576808e-01],\n",
       "         ...,\n",
       "         [0.00000000e+00, 3.79016881e-26, 1.67798848e-17, ...,\n",
       "          8.14378842e-11, 8.33930075e-03, 5.16352654e-01],\n",
       "         [7.86163751e-03, 1.30784737e-31, 2.93620803e-28, ...,\n",
       "          7.29838912e-34, 0.00000000e+00, 0.00000000e+00],\n",
       "         [3.81371492e-27, 1.85210276e-19, 1.56855917e-11, ...,\n",
       "          2.01895568e-06, 6.12508915e-02, 5.02247930e-01]]],\n",
       "\n",
       "\n",
       "       [[[2.37044629e-12, 5.26649237e-01, 3.85020148e-29, ...,\n",
       "          5.64312977e-28, 1.27248351e-22, 1.47314763e-22],\n",
       "         [0.00000000e+00, 4.80567489e-29, 1.27657047e-17, ...,\n",
       "          2.09126028e-09, 4.46930202e-03, 6.94065332e-01],\n",
       "         [0.00000000e+00, 2.23826749e-31, 6.52297674e-18, ...,\n",
       "          3.80107856e-09, 8.31164513e-03, 6.72837675e-01],\n",
       "         ...,\n",
       "         [3.15290013e-31, 0.00000000e+00, 5.45383573e-01, ...,\n",
       "          8.90864152e-03, 5.15125730e-14, 1.37948611e-18],\n",
       "         [1.35091241e-29, 0.00000000e+00, 5.26859641e-01, ...,\n",
       "          8.08899477e-03, 4.43128173e-13, 2.93112807e-17],\n",
       "         [5.48602344e-11, 5.45399010e-01, 3.28805484e-26, ...,\n",
       "          3.87687911e-25, 7.22959410e-20, 3.99020101e-20]]],\n",
       "\n",
       "\n",
       "       [[[3.04687263e-22, 3.07804140e-29, 3.57246369e-01, ...,\n",
       "          4.26748060e-02, 1.20485106e-08, 1.03600756e-11],\n",
       "         [0.00000000e+00, 4.53449324e-28, 1.32226692e-15, ...,\n",
       "          9.47515773e-08, 1.90667193e-02, 4.16994482e-01],\n",
       "         [1.15086915e-29, 0.00000000e+00, 7.24795043e-01, ...,\n",
       "          2.22131032e-02, 3.48308622e-13, 2.18751453e-17],\n",
       "         ...,\n",
       "         [2.34153094e-28, 7.54567741e-13, 5.77260140e-19, ...,\n",
       "          1.13300218e-15, 2.52976838e-06, 6.92291796e-05],\n",
       "         [3.73237059e-02, 1.04411937e-08, 2.18368316e-26, ...,\n",
       "          3.24147414e-27, 8.63566148e-30, 5.57689691e-31],\n",
       "         [1.01059046e-15, 2.49732938e-03, 5.00804482e-20, ...,\n",
       "          9.14985752e-18, 2.73055828e-10, 2.32493580e-09]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[1.24793851e-33, 4.96170780e-25, 3.83840307e-15, ...,\n",
       "          8.05523683e-08, 4.48946701e-03, 8.98052335e-01],\n",
       "         [3.36044699e-22, 5.95962629e-02, 0.00000000e+00, ...,\n",
       "          0.00000000e+00, 5.97922443e-31, 3.01998153e-30],\n",
       "         [2.07358203e-25, 1.85116669e-08, 2.60767476e-21, ...,\n",
       "          2.96159032e-18, 2.30640378e-08, 2.62301768e-07],\n",
       "         ...,\n",
       "         [0.00000000e+00, 1.62089166e-36, 2.98391276e-23, ...,\n",
       "          3.98803690e-12, 1.05437932e-04, 9.92869794e-01],\n",
       "         [5.48352348e-03, 1.07962426e-13, 2.34591219e-37, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [1.50342621e-26, 2.78874419e-24, 7.34327568e-06, ...,\n",
       "          6.96461871e-02, 9.21601057e-01, 6.14771061e-03]]],\n",
       "\n",
       "\n",
       "       [[[9.03765607e-33, 6.17978904e-26, 3.04116731e-13, ...,\n",
       "          3.88662238e-06, 2.62251515e-02, 8.91820967e-01],\n",
       "         [3.72394288e-21, 4.86603130e-05, 2.46250424e-20, ...,\n",
       "          8.79572909e-18, 5.34627231e-09, 1.55403068e-08],\n",
       "         [1.44515706e-25, 2.65136350e-05, 3.57728216e-29, ...,\n",
       "          9.39886723e-26, 4.32290109e-14, 2.40996329e-12],\n",
       "         ...,\n",
       "         [5.04788742e-28, 0.00000000e+00, 3.89718920e-01, ...,\n",
       "          6.68676058e-03, 2.19648306e-15, 2.41279227e-19],\n",
       "         [1.75412616e-03, 2.85777187e-33, 1.63811566e-33, ...,\n",
       "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "         [2.90369850e-32, 1.49202517e-23, 1.15027769e-14, ...,\n",
       "          9.36861397e-08, 7.56992633e-03, 8.65533233e-01]]],\n",
       "\n",
       "\n",
       "       [[[7.69944084e-28, 5.61361922e-20, 2.42058543e-12, ...,\n",
       "          1.09391692e-06, 1.46360369e-02, 5.05644441e-01],\n",
       "         [0.00000000e+00, 1.52482350e-32, 1.72553929e-17, ...,\n",
       "          2.14068976e-08, 1.17627354e-02, 8.15569997e-01],\n",
       "         [2.20169387e-31, 0.00000000e+00, 6.15166843e-01, ...,\n",
       "          1.30119454e-02, 1.77433778e-13, 7.54036588e-18],\n",
       "         ...,\n",
       "         [4.89239156e-01, 6.50534762e-26, 1.43515369e-26, ...,\n",
       "          3.15959081e-31, 0.00000000e+00, 0.00000000e+00],\n",
       "         [0.00000000e+00, 7.85264081e-30, 2.34951761e-18, ...,\n",
       "          1.32789502e-09, 4.06950992e-03, 7.91117728e-01],\n",
       "         [7.81801880e-12, 4.53828245e-01, 2.79176661e-27, ...,\n",
       "          2.28493731e-26, 1.11631505e-20, 6.01708464e-21]]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = probability_model.predict(test_f)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = predictions.numpy().reshape(50000,20)\n",
    "test = mt.get_results(test)\n",
    "label = label.numpy().reshape(50000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = tf.math.confusion_matrix(test, label).numpy()\n",
    "cf = pd.DataFrame(cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = mt.batch_result(cf)\n",
    "result = result.to_csv('test1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./test2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.divide_Result(cf, 'test', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
