{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.io as sc\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe582e6bda0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                     input_shape=[28, 28, 1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model\n",
    "\n",
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((7, 7, 256)))\n",
    "    assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 7, 7, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 14, 14, 64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 28, 28, 1)\n",
    "\n",
    "    return model\n",
    "\n",
    "generator = make_generator_model()\n",
    "\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.00063982]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_image)\n",
    "print (decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 2, 1)\n"
     ]
    }
   ],
   "source": [
    "def make_generator(blockSize):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(16, (1, 2), strides=(1, 2), activation=\"linear\",\n",
    "                            input_shape=(blockSize, 2, 1)))\n",
    "    model.add(layers.Conv2D(32, (1, 16), activation=\"linear\", padding='same'))\n",
    "    model.add(layers.Conv2D(16, (1, 32), activation=\"linear\", padding='same'))\n",
    "    model.add(layers.Reshape((blockSize, 16, 1)))\n",
    "    model.add(layers.AveragePooling2D((1, 8)))\n",
    "    model.add(layers.Dense(1))\n",
    "    return model\n",
    "\n",
    "def make_discriminator_model(blockSize):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (2, 1), strides=(1, 1), padding='same',\n",
    "                            input_shape=[blockSize, 2, 1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "    return model\n",
    "\n",
    "g_noise = tf.random.normal([1, 50, 2, 1])\n",
    "generator = make_generator(50)\n",
    "test = generator(g_noise)\n",
    "print(test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.00025338]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "discrminator = make_discriminator_model(50)\n",
    "decision = discrminator(test)\n",
    "print(decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 50, 2, 64)         192       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 50, 2, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 50, 2, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 25, 1, 128)        204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 25, 1, 128)        0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 25, 1, 128)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 3201      \n",
      "=================================================================\n",
      "Total params: 208,321\n",
      "Trainable params: 208,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "test = make_discriminator_model(50)\n",
    "test.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.70710678+0.70710678j  0.70710678-0.70710678j -0.70710678+0.70710678j\n",
      " -0.70710678-0.70710678j] \n",
      " [-0.9486833 +0.9486833j  -0.31622777+0.9486833j   0.31622777+0.9486833j\n",
      "  0.9486833 +0.9486833j  -0.9486833 +0.31622777j -0.31622777+0.31622777j\n",
      "  0.31622777+0.31622777j  0.9486833 +0.31622777j -0.9486833 -0.31622777j\n",
      " -0.31622777-0.31622777j  0.31622777-0.31622777j  0.9486833 -0.31622777j\n",
      " -0.9486833 -0.9486833j  -0.31622777-0.9486833j   0.31622777-0.9486833j\n",
      "  0.9486833 -0.9486833j ]\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "cons_4 = np.dot(np.sqrt(0.5),[complex(i,j)for i in c_4 for j in c_4])\n",
    "cons_16 = np.array([complex(i,j)for j in c_16 for i in c_16r])\n",
    "cons_16 = cons_16/np.sqrt(np.mean(np.abs(cons_16)**2))\n",
    "print(cons_4,\"\\n\",cons_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset(dataFile, labelFile):\n",
    "    dataFile = \"./communication/\" + dataFile\n",
    "    labelFile = \"./communication/\" + labelFile\n",
    "    my_data = sc.loadmat(dataFile)\n",
    "    my_labels = sc.loadmat(labelFile)\n",
    "    my_data = my_data['Y']\n",
    "    X = my_labels['L_S_x']\n",
    "    myOrig = table_data(my_data, my_labels['L_Constellations'][0], X)\n",
    "    mytable = assign_label(myOrig)\n",
    "    return mytable\n",
    "\n",
    "\n",
    "def assign_label(data):\n",
    "    c_4 = [1,-1]\n",
    "    c_16 = [3,1,-1,-3]\n",
    "    c_16r = [-3,-1,1,3]\n",
    "    cons_4 = np.dot(np.sqrt(0.5),[complex(i,j)for i in c_4 for j in c_4])\n",
    "    cons_16 = np.array([complex(i,j)for j in c_16 for i in c_16r])\n",
    "    cons_16 = cons_16/np.sqrt(np.mean(np.abs(cons_16)**2))\n",
    "    cons4 = data[data.cons==1]\n",
    "    cons4_label = np.array([[cons_4[i-1]]for i in cons4.label])\n",
    "    cons16 = data[data.cons==2]\n",
    "    cons16_label = np.array([[cons_16[i-1]]for i in cons16.label.to_numpy().real.astype(int)])\n",
    "    data[data.cons==2].index\n",
    "    data['buffer'] = 0\n",
    "    data['buffer'] = 0\n",
    "    data.iloc[data[data.cons==1].index, 5] = cons4_label\n",
    "    data.iloc[data[data.cons==2].index, 5] = cons16_label\n",
    "    data['label_real'] = data.buffer.to_numpy().real\n",
    "    data['label_imag'] = data.buffer.to_numpy().imag\n",
    "    myTest = data.copy()\n",
    "    myTest.loc[myTest.cons == 2, 'label'] = myTest.loc[myTest.cons == 2, 'label'] + 4\n",
    "    myTest.label = myTest.label - 1\n",
    "    return myTest\n",
    "\n",
    "\n",
    "def table_data(my_data, cons, label):\n",
    "    block = my_data.shape[1]\n",
    "    my_data_size = my_data.shape[0] * block\n",
    "    my_data_div = my_data.T.reshape(my_data_size, )\n",
    "    cons_array = np.array([[cons[i]] * my_data.shape[0] for i in range(0, block)]).reshape(my_data_size, )\n",
    "    block_array = np.array([([i + 1] * my_data.shape[0]) for i in range(0, block)]).reshape(my_data_size, )\n",
    "    label_array = label.T.reshape(my_data_size, )    \n",
    "    test_pd = pd.DataFrame({'real': my_data_div.real, 'imag': my_data_div.imag,\n",
    "                            'cons': cons_array, 'block': block_array,\n",
    "                            'label': label_array})\n",
    "    return test_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>real</th>\n",
       "      <th>imag</th>\n",
       "      <th>cons</th>\n",
       "      <th>block</th>\n",
       "      <th>label</th>\n",
       "      <th>buffer</th>\n",
       "      <th>label_real</th>\n",
       "      <th>label_imag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48.806980</td>\n",
       "      <td>-13.804771</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.948683-0.316228j</td>\n",
       "      <td>0.948683</td>\n",
       "      <td>-0.316228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-21.419588</td>\n",
       "      <td>-58.747407</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0.316228-0.948683j</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>-0.948683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61.033908</td>\n",
       "      <td>40.251547</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.948683+0.948683j</td>\n",
       "      <td>0.948683</td>\n",
       "      <td>0.948683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25.841522</td>\n",
       "      <td>72.678677</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.316228+0.948683j</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.948683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40.576088</td>\n",
       "      <td>67.132076</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.948683+0.948683j</td>\n",
       "      <td>0.948683</td>\n",
       "      <td>0.948683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>28.789324</td>\n",
       "      <td>-41.060747</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>18</td>\n",
       "      <td>0.316228-0.948683j</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>-0.948683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>28.106421</td>\n",
       "      <td>-54.533358</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>18</td>\n",
       "      <td>0.316228-0.948683j</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>-0.948683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>-19.969952</td>\n",
       "      <td>-39.392252</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.316228-0.316228j</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>-0.316228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>-14.243322</td>\n",
       "      <td>5.572663</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>13</td>\n",
       "      <td>-0.316228-0.316228j</td>\n",
       "      <td>-0.316228</td>\n",
       "      <td>-0.316228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>33.826814</td>\n",
       "      <td>41.404952</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.316228+0.948683j</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.948683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            real       imag  cons  block  label              buffer  \\\n",
       "0      48.806980 -13.804771     2      1     15  0.948683-0.316228j   \n",
       "1     -21.419588 -58.747407     2      1     18  0.316228-0.948683j   \n",
       "2      61.033908  40.251547     2      1      7  0.948683+0.948683j   \n",
       "3      25.841522  72.678677     2      1      6  0.316228+0.948683j   \n",
       "4      40.576088  67.132076     2      1      7  0.948683+0.948683j   \n",
       "...          ...        ...   ...    ...    ...                 ...   \n",
       "49995  28.789324 -41.060747     2   1000     18  0.316228-0.948683j   \n",
       "49996  28.106421 -54.533358     2   1000     18  0.316228-0.948683j   \n",
       "49997 -19.969952 -39.392252     2   1000     14  0.316228-0.316228j   \n",
       "49998 -14.243322   5.572663     2   1000     13 -0.316228-0.316228j   \n",
       "49999  33.826814  41.404952     2   1000      6  0.316228+0.948683j   \n",
       "\n",
       "       label_real  label_imag  \n",
       "0        0.948683   -0.316228  \n",
       "1        0.316228   -0.948683  \n",
       "2        0.948683    0.948683  \n",
       "3        0.316228    0.948683  \n",
       "4        0.948683    0.948683  \n",
       "...           ...         ...  \n",
       "49995    0.316228   -0.948683  \n",
       "49996    0.316228   -0.948683  \n",
       "49997    0.316228   -0.316228  \n",
       "49998   -0.316228   -0.316228  \n",
       "49999    0.316228    0.948683  \n",
       "\n",
       "[50000 rows x 8 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = \"hard\"\n",
    "data1_label = \"hard_label\"\n",
    "data = dataset(data1, data1_label)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(20, use_bias=False, input_shape=[2,]))\n",
    "    model.add(layers.Dense(50, activation='relu'))\n",
    "    model.add(layers.Dense(2))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_s = generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_n = generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_i = generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(20, use_bias=False, input_shape=[2,]))\n",
    "    model.add(layers.Dense(50, activation = 'sigmoid'))\n",
    "    model.add(layers.Dense(1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_t = make_discriminator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_d = make_discriminator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_loss(real, fake):\n",
    "    loss = tf.reduce_mean(tf.abs(real - fake))\n",
    "    return LAMBDA * 0.5 * loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_s_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "generator_n_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "generator_i_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "discriminator_d_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_t_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/dis\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(generator_s=generator_s,\n",
    "                           generator_n=generator_n,\n",
    "                           generator_i=generator_i,\n",
    "                           discriminator_t=discriminator_t,\n",
    "                           discriminator_d=discriminator_d,\n",
    "                           generator_s_optimizer=generator_s_optimizer,\n",
    "                           generator_n_optimizer=generator_n_optimizer,\n",
    "                           generator_i_optimizer=generator_i_optimizer,\n",
    "                           discriminator_d_optimizer=discriminator_d_optimizer,\n",
    "                           discriminator_t_optimizer=discriminator_t_optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6b053243d9a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpersistent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator_s\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def train_step(total, label):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        s = generator_s(total, training=True)\n",
    "        n = generator_n(total, training=True)\n",
    "        i = generator_i(total, training=True)\n",
    "        gen = s + n + i\n",
    "        fake_t = discriminator_t(gen, training=True)\n",
    "        real_t = discriminator_t(total, training=True)\n",
    "        fake_d = discriminator_d(s, training=True)\n",
    "        real_d = discriminator_d(label, training=True)\n",
    "        s_loss = generator_loss(s)\n",
    "        n_loss = generator_loss(n)\n",
    "        i_loss = generator_loss(i)\n",
    "        disc_t_loss = discriminator_loss(real_t, fake_t)\n",
    "        disc_d_loss = discriminator_loss(real_d, fake_d)\n",
    "        identity_s_loss = identity_loss(label, s)\n",
    "        identity_g_loss = identity_loss(total, gen)\n",
    "        total_s_loss = 0.5 * (identity_s_loss + s_loss) + 0.5 * (identity_g_loss)\n",
    "        total_n_loss = identity_g_loss + n_loss\n",
    "        total_i_loss = identity_g_loss + i_loss\n",
    "        print()\n",
    "        \n",
    "    gradients_of_s_generator = tape.gradient(total_s_loss, generator_s.trainable_variables)\n",
    "    gradients_of_i_generator = tape.gradient(total_i_loss, generator_i.trainable_variables)\n",
    "    gradients_of_n_generator = tape.gradient(total_n_loss, generator_n.trainable_variables)\n",
    "    gradients_of_discriminator_t = tape.gradient(disc_t_loss, discriminator_t.trainable_variables)\n",
    "    gradients_of_discriminator_d = tape.gradient(disc_d_loss, discriminator_d.trainable_variables)\n",
    "    generator_s_optimizer.apply_gradients(zip(gradients_of_s_generator, generator_s.trainable_variables))\n",
    "    generator_i_optimizer.apply_gradients(zip(gradients_of_i_generator, generator_i.trainable_variables))\n",
    "    generator_n_optimizer.apply_gradients(zip(gradients_of_n_generator, generator_n.trainable_variables))\n",
    "    discriminator_t_optimizer.apply_gradients(zip(gradients_of_discriminator_t, discriminator_t.trainable_variables))\n",
    "    discriminator_d_optimizer.apply_gradients(zip(gradients_of_discriminator_d, discriminator_d.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature = data.loc[:,('real', 'imag')]\n",
    "train_label = data.loc[:, ('label_real', 'label_imag')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feature = tf.cast(train_feature, tf.float32)\n",
    "test_label = tf.cast(train_label, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "....."
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    n = 0\n",
    "    for i in range(len(test_feature)):\n",
    "        test = tf.reshape(test_feature[i], [1,2])\n",
    "        label = tf.reshape(test_label[i], [1,2])\n",
    "        train_step(test, label)\n",
    "        if n % 10 == 0:\n",
    "            print ('.', end='')\n",
    "            n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = generator_s(test_feature, training=False)\n",
    "i = generator_i(test_feature, training=False)\n",
    "n = generator_n(test_feature, training=False)\n",
    "gen = s + i + n\n",
    "test = np.array([(test_feature - gen)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([s - test_label]).mean()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile_model(norm):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, input_h\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(20)\n",
    "    ])\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50000, 2), dtype=float32, numpy=\n",
       "array([[ 0.74356425, -0.39451778],\n",
       "       [-0.63942367, -0.6913041 ],\n",
       "       [ 0.69628894,  0.9595338 ],\n",
       "       ...,\n",
       "       [-0.59323114, -0.5585426 ],\n",
       "       [-0.6560704 ,  0.2876377 ],\n",
       "       [ 0.56631446,  0.91844213]], dtype=float32)>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator_s(train_feature.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = real.size\n",
    "real = np.array([s[:, 0]]).reshape(size)\n",
    "imag = np.array([s[:, 1]]).reshape(size)\n",
    "mlcData = pd.DataFrame({'real':real, 'imag':imag.T, 'label':symbol})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import os,sys\n",
    "import scipy.io as sc\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import math\n",
    "def dataset(dataFile, labelFile):\n",
    "    dataFile = \"../ML_Symbol_Gen-main/\" + dataFile\n",
    "    labelFile = \"../ML_Symbol_Gen-main/\" + labelFile\n",
    "    my_data = sc.loadmat(dataFile)\n",
    "    my_labels = sc.loadmat(labelFile)\n",
    "    my_data = my_data['Y']\n",
    "    X = my_labels['L_S_x']\n",
    "    myOrig = table_data(my_data, my_labels['L_Constellations'][0], X)\n",
    "    mytable = assign_label(myOrig)\n",
    "    return mytable\n",
    "\n",
    "\n",
    "def assign_label(data):\n",
    "    c_4 = [1,-1]\n",
    "    c_16 = [3,1,-1,-3]\n",
    "    c_16r = [-3,-1,1,3]\n",
    "    cons_4 = np.dot(np.sqrt(0.5),[complex(i,j)for i in c_4 for j in c_4])\n",
    "    cons_16 = np.array([complex(i,j)for j in c_16 for i in c_16r])\n",
    "    cons_16 = cons_16/np.sqrt(np.mean(np.abs(cons_16)**2))\n",
    "    cons4 = data[data.cons==1]\n",
    "    cons4_label = np.array([[cons_4[i-1]]for i in cons4.label])\n",
    "    cons16 = data[data.cons==2]\n",
    "    cons16_label = np.array([[cons_16[i-1]]for i in cons16.label.to_numpy().real.astype(int)])\n",
    "    data[data.cons==2].index\n",
    "    data['buffer'] = 0\n",
    "    data['buffer'] = 0\n",
    "    data.iloc[data[data.cons==1].index, 5] = cons4_label\n",
    "    data.iloc[data[data.cons==2].index, 5] = cons16_label\n",
    "    data['label_real'] = data.buffer.to_numpy().real\n",
    "    data['label_imag'] = data.buffer.to_numpy().imag\n",
    "    myTest = data.copy()\n",
    "    myTest.loc[myTest.cons == 2, 'label'] = myTest.loc[myTest.cons == 2, 'label'] + 4\n",
    "    myTest.label = myTest.label - 1\n",
    "    return myTest\n",
    "\n",
    "\n",
    "def table_data(my_data, cons, label):\n",
    "    block = my_data.shape[1]\n",
    "    my_data_size = my_data.shape[0] * block\n",
    "    my_data_div = my_data.T.reshape(my_data_size, )\n",
    "    cons_array = np.array([[cons[i]] * my_data.shape[0] for i in range(0, block)]).reshape(my_data_size, )\n",
    "    block_array = np.array([([i + 1] * my_data.shape[0]) for i in range(0, block)]).reshape(my_data_size, )\n",
    "    label_array = label.T.reshape(my_data_size, )\n",
    "    test_pd = pd.DataFrame({'real': my_data_div.real, 'imag': my_data_div.imag,\n",
    "                            'cons': cons_array, 'block': block_array,\n",
    "                            'label': label_array})\n",
    "    return test_pd\n",
    "\n",
    "\n",
    "def make_generator():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(2, use_bias=False, input_shape=[50,2]))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Reshape((50, 2, 1)))\n",
    "    model.add(layers.Conv2DTranspose(128, (2, 1), strides=(1, 1), padding='same', use_bias=False))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.add(layers.Reshape((50,2)))\n",
    "    return model\n",
    "\n",
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Reshape((50, 2, 1)))\n",
    "    model.add(layers.Conv2D(64, (2, 1), strides=(1, 1), padding='same',\n",
    "                                     input_shape=[1, 50, 2]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "    return model\n",
    "\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "def identity_loss(real, fake):\n",
    "    loss = tf.reduce_mean(tf.abs(real - fake))\n",
    "    return LAMBDA * 0.5 * loss\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(total, label):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        s = generator_s(total, training=True)\n",
    "        n = generator_n(total, training=True)\n",
    "        i = generator_i(total, training=True)\n",
    "        gen = (s + n + i)\n",
    "        gen = tf.reshape(gen, (1,50,2))\n",
    "        fake_t = discriminator_t(gen, training=True)\n",
    "        real_t = discriminator_t(total, training=True)\n",
    "        fake_d = discriminator_d(s, training=True)\n",
    "        real_d = discriminator_d(label, training=True)\n",
    "        gen_loss = generator_loss(fake_t)\n",
    "        gen_s_loss = generator_loss(fake_d)\n",
    "        disc_t_loss = discriminator_loss(real_t, fake_t)\n",
    "        disc_d_loss = discriminator_loss(real_d, fake_d)\n",
    "        identity_s_loss = identity_loss(label, s)\n",
    "        identity_g_loss = identity_loss(total, gen)\n",
    "        identity_total_loss = identity_g_loss + identity_s_loss\n",
    "        total_s_loss = identity_total_loss + 1/2 * gen_loss  + gen_s_loss + identity_s_loss\n",
    "        total_n_loss = identity_total_loss + gen_loss\n",
    "        total_i_loss = identity_total_loss + gen_loss\n",
    "\n",
    "    gradients_of_s_generator = tape.gradient(total_s_loss, generator_s.trainable_variables)\n",
    "    gradients_of_i_generator = tape.gradient(total_i_loss, generator_i.trainable_variables)\n",
    "    gradients_of_n_generator = tape.gradient(total_n_loss, generator_n.trainable_variables)\n",
    "    gradients_of_discriminator_t = tape.gradient(disc_t_loss, discriminator_t.trainable_variables)\n",
    "    gradients_of_discriminator_d = tape.gradient(disc_d_loss, discriminator_d.trainable_variables)\n",
    "    generator_s_optimizer.apply_gradients(zip(gradients_of_s_generator, generator_s.trainable_variables))\n",
    "    generator_i_optimizer.apply_gradients(zip(gradients_of_i_generator, generator_i.trainable_variables))\n",
    "    generator_n_optimizer.apply_gradients(zip(gradients_of_n_generator, generator_n.trainable_variables))\n",
    "    discriminator_t_optimizer.apply_gradients(zip(gradients_of_discriminator_t, discriminator_t.trainable_variables))\n",
    "    discriminator_d_optimizer.apply_gradients(zip(gradients_of_discriminator_d, discriminator_d.trainable_variables))\n",
    "\n",
    "def shuffle_data(my_table):\n",
    "    '''\n",
    "    real_y = (2*my_table.real.min())/(my_table.real.max() - my_table.real.min()) + 1\n",
    "    real_x = (my_table.real.max()) / (1 + real_y)\n",
    "    imag_y = (2*my_table.imag.min())/(my_table.imag.max() - my_table.imag.min()) + 1\n",
    "    imag_x = (my_table.imag.max()) / (1 + imag_y)\n",
    "    my_table.real = (my_table.real / real_x) - real_y\n",
    "    my_table.imag = (my_table.imag/ imag_x) - imag_y\n",
    "    '''\n",
    "    train_feature = data.loc[:, ('real', 'imag')]\n",
    "    train_label = data.loc[:, ('label_real', 'label_imag')]\n",
    "    test_feature = tf.cast(train_feature, tf.float32)\n",
    "    test_label = tf.cast(train_label, tf.float32)\n",
    "    test_feature = tf.reshape(test_feature,(1000,1,50,2))\n",
    "    test_label = tf.reshape(test_label, (1000,1,50,2))\n",
    "    symbol = data.loc[:, 'label']\n",
    "    return test_feature, test_label\n",
    "\n",
    "generator_s = make_generator()\n",
    "generator_n = make_generator()\n",
    "generator_i = make_generator()\n",
    "discriminator_t = make_discriminator_model()\n",
    "discriminator_d = make_discriminator_model()\n",
    "\n",
    "\n",
    "generator_s_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "generator_n_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "generator_i_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "discriminator_d_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_t_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "checkpoint_path = \"./checkpoints/cnn\"\n",
    "ckpt = tf.train.Checkpoint(generator_s=generator_s,\n",
    "                           generator_n=generator_n,\n",
    "                           generator_i=generator_i,\n",
    "                           discriminator_t=discriminator_t,\n",
    "                           discriminator_d=discriminator_d,\n",
    "                           generator_s_optimizer=generator_s_optimizer,\n",
    "                           generator_n_optimizer=generator_n_optimizer,\n",
    "                           generator_i_optimizer=generator_i_optimizer,\n",
    "                           discriminator_d_optimizer=discriminator_d_optimizer,\n",
    "                           discriminator_t_optimizer=discriminator_t_optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')\n",
    "LAMBDA = 10\n",
    "EPOCHS = 50\n",
    "data1 = \"my_data\"\n",
    "data1_label = \"my_labels\"\n",
    "data = dataset(data1, data1_label)\n",
    "file_directory = './result/tes2/'\n",
    "f, l = shuffle_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = generator_i(f[0], training=False)\n",
    "n = generator_n(f[0], training=False)\n",
    "s = generator_s(f[0], training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 50, 2), dtype=float32, numpy=\n",
       "array([[[-0.01165958,  0.04710186],\n",
       "        [-0.04699365,  0.04541421],\n",
       "        [ 0.0286256 ,  0.11764488],\n",
       "        [ 0.04087699,  0.08862203],\n",
       "        [ 0.02970773,  0.04105288],\n",
       "        [-0.03506368,  0.00696408],\n",
       "        [ 0.04236872,  0.08455611],\n",
       "        [ 0.0341803 ,  0.07166582],\n",
       "        [-0.0529087 ,  0.01400566],\n",
       "        [-0.14058375, -0.09424768],\n",
       "        [ 0.0151098 ,  0.09260677],\n",
       "        [ 0.04741369,  0.05953749],\n",
       "        [-0.1777518 , -0.06442233],\n",
       "        [-0.17516671, -0.11706606],\n",
       "        [-0.06003933, -0.05898825],\n",
       "        [ 0.01646209,  0.07246798],\n",
       "        [ 0.03749654,  0.06943575],\n",
       "        [-0.16360986,  0.04353789],\n",
       "        [ 0.01007734,  0.05163335],\n",
       "        [-0.10028578, -0.0232053 ],\n",
       "        [ 0.01163929,  0.09862007],\n",
       "        [-0.12342912,  0.00427034],\n",
       "        [-0.1600461 , -0.00927445],\n",
       "        [-0.04776413,  0.07008516],\n",
       "        [ 0.02773576,  0.06955869],\n",
       "        [-0.10547085, -0.00927497],\n",
       "        [ 0.00997397,  0.07738324],\n",
       "        [ 0.05829114,  0.11171931],\n",
       "        [-0.03821768,  0.06839689],\n",
       "        [-0.18424316, -0.03057099],\n",
       "        [ 0.00371523, -0.01892659],\n",
       "        [ 0.04908825,  0.03761617],\n",
       "        [-0.12994769, -0.10239467],\n",
       "        [ 0.00373906,  0.0635483 ],\n",
       "        [-0.08720475,  0.05613215],\n",
       "        [ 0.02227315,  0.04443428],\n",
       "        [-0.0925844 ,  0.06575475],\n",
       "        [-0.14893949, -0.09555198],\n",
       "        [-0.04477195,  0.05610795],\n",
       "        [ 0.02580728,  0.06497019],\n",
       "        [-0.11389601,  0.04288847],\n",
       "        [-0.07150535,  0.05099155],\n",
       "        [ 0.01988221,  0.08953626],\n",
       "        [-0.09603102,  0.02165951],\n",
       "        [-0.08007205, -0.005918  ],\n",
       "        [ 0.01933923,  0.03536415],\n",
       "        [-0.12582628, -0.09176184],\n",
       "        [ 0.01074331,  0.05927698],\n",
       "        [-0.18007287, -0.04340991],\n",
       "        [-0.00248978,  0.07709638]]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(2, use_bias=False, input_shape=[50,2]))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Reshape((50, 2, 1)))\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    model.add(layers.Reshape((50,2)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_test = make_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 50, 2), dtype=float32, numpy=\n",
       "array([[[-4.7518844e-03, -1.4423817e-02],\n",
       "        [-6.0334550e-03, -5.9363381e-03],\n",
       "        [-6.2133595e-03, -9.8154284e-03],\n",
       "        [-3.6675632e-03, -9.9786269e-03],\n",
       "        [ 6.6237049e-03, -9.8263128e-03],\n",
       "        [-1.4844697e-03, -1.2666809e-02],\n",
       "        [-3.0093305e-02,  1.7870571e-02],\n",
       "        [-3.0743811e-02, -3.0736204e-03],\n",
       "        [ 1.3923310e-02, -1.3344743e-03],\n",
       "        [ 1.5177790e-03, -4.9369223e-03],\n",
       "        [-1.8499680e-02, -1.1614773e-02],\n",
       "        [-2.4047339e-02,  1.8808618e-04],\n",
       "        [ 5.1625781e-03, -1.6651477e-03],\n",
       "        [ 9.9959970e-03,  1.6799193e-03],\n",
       "        [-1.8128458e-02,  7.5068274e-03],\n",
       "        [ 1.7495800e-03, -6.4042830e-03],\n",
       "        [-1.2254220e-02, -8.0267177e-04],\n",
       "        [ 5.0711455e-03,  4.6894024e-03],\n",
       "        [ 3.7442453e-03, -1.3466846e-02],\n",
       "        [ 6.6979637e-04,  5.9850411e-03],\n",
       "        [-1.2132203e-02, -1.1426675e-03],\n",
       "        [-1.5352142e-02, -4.7459546e-03],\n",
       "        [ 4.7154925e-03,  1.2678334e-02],\n",
       "        [ 6.4445883e-03,  2.0342943e-04],\n",
       "        [-8.2811574e-03, -5.4595282e-04],\n",
       "        [-2.9145672e-03,  1.4594439e-03],\n",
       "        [-8.1313951e-03, -1.8566033e-02],\n",
       "        [-2.8189417e-02, -1.8122606e-02],\n",
       "        [-2.3630941e-03, -1.4190579e-02],\n",
       "        [ 1.5243972e-03, -1.8669717e-02],\n",
       "        [-3.2637697e-03,  7.0774769e-03],\n",
       "        [-2.0801138e-02, -3.6525941e-03],\n",
       "        [-2.0759804e-02,  5.9723379e-03],\n",
       "        [-1.8223519e-02,  3.6116205e-03],\n",
       "        [ 2.5466862e-03,  5.9397724e-03],\n",
       "        [ 5.1288716e-03, -6.1330288e-03],\n",
       "        [ 1.5079106e-02, -9.6076913e-03],\n",
       "        [-1.0383257e-02,  9.1698989e-03],\n",
       "        [-1.1637948e-02,  8.7743793e-03],\n",
       "        [-6.8564899e-05, -7.2174505e-03],\n",
       "        [ 2.2787254e-03, -6.0980949e-03],\n",
       "        [ 2.7172321e-03, -8.0779977e-03],\n",
       "        [-4.2024823e-03, -9.3691964e-03],\n",
       "        [-2.4870313e-03, -3.3489775e-03],\n",
       "        [ 1.3459975e-02, -8.9253252e-03],\n",
       "        [-7.5582652e-03,  6.4608194e-03],\n",
       "        [-1.0111212e-02,  5.4570278e-03],\n",
       "        [-9.4991205e-03,  1.3174746e-02],\n",
       "        [-1.1807247e-02,  4.2184712e-03],\n",
       "        [ 7.3042139e-04,  9.7565744e-03]]], dtype=float32)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator_test(f[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
