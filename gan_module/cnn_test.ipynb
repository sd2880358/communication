{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import os,sys\n",
    "import scipy.io as sc\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import math\n",
    "def dataset(dataFile, labelFile):\n",
    "    dataFile = \"../ML_Symbol_Gen-main/\" + dataFile\n",
    "    labelFile = \"../ML_Symbol_Gen-main/\" + labelFile\n",
    "    my_data = sc.loadmat(dataFile)\n",
    "    my_labels = sc.loadmat(labelFile)\n",
    "    my_data = my_data['Y']\n",
    "    X = my_labels['L_S_x']\n",
    "    myOrig = table_data(my_data, my_labels['L_Constellations'][0], X)\n",
    "    mytable = assign_label(myOrig)\n",
    "    return mytable\n",
    "\n",
    "\n",
    "def assign_label(data):\n",
    "    c_4 = [1,-1]\n",
    "    c_16 = [3,1,-1,-3]\n",
    "    c_16r = [-3,-1,1,3]\n",
    "    cons_4 = np.dot(np.sqrt(0.5),[complex(i,j)for i in c_4 for j in c_4])\n",
    "    cons_16 = np.array([complex(i,j)for j in c_16 for i in c_16r])\n",
    "    cons_16 = cons_16/np.sqrt(np.mean(np.abs(cons_16)**2))\n",
    "    cons4 = data[data.cons==1]\n",
    "    cons4_label = np.array([[cons_4[i-1]]for i in cons4.label])\n",
    "    cons16 = data[data.cons==2]\n",
    "    cons16_label = np.array([[cons_16[i-1]]for i in cons16.label.to_numpy().real.astype(int)])\n",
    "    data[data.cons==2].index\n",
    "    data['buffer'] = 0\n",
    "    data['buffer'] = 0\n",
    "    data.iloc[data[data.cons==1].index, 5] = cons4_label\n",
    "    data.iloc[data[data.cons==2].index, 5] = cons16_label\n",
    "    data['label_real'] = data.buffer.to_numpy().real\n",
    "    data['label_imag'] = data.buffer.to_numpy().imag\n",
    "    myTest = data.copy()\n",
    "    myTest.loc[myTest.cons == 2, 'label'] = myTest.loc[myTest.cons == 2, 'label'] + 4\n",
    "    myTest.label = myTest.label - 1\n",
    "    return myTest\n",
    "\n",
    "\n",
    "def table_data(my_data, cons, label):\n",
    "    block = my_data.shape[1]\n",
    "    my_data_size = my_data.shape[0] * block\n",
    "    my_data_div = my_data.T.reshape(my_data_size, )\n",
    "    cons_array = np.array([[cons[i]] * my_data.shape[0] for i in range(0, block)]).reshape(my_data_size, )\n",
    "    block_array = np.array([([i + 1] * my_data.shape[0]) for i in range(0, block)]).reshape(my_data_size, )\n",
    "    label_array = label.T.reshape(my_data_size, )\n",
    "    test_pd = pd.DataFrame({'real': my_data_div.real, 'imag': my_data_div.imag,\n",
    "                            'cons': cons_array, 'block': block_array,\n",
    "                            'label': label_array})\n",
    "    return test_pd\n",
    "\n",
    "\n",
    "def make_generator():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(2, use_bias=False, input_shape=[50,2]))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Reshape((50, 2, 1)))\n",
    "    model.add(layers.Conv2DTranspose(128, (2, 1), strides=(1, 1), padding='same', use_bias=False))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.add(layers.Reshape((50,2)))\n",
    "    return model\n",
    "\n",
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Reshape((50, 2, 1)))\n",
    "    model.add(layers.Conv2D(64, (2, 1), strides=(1, 1), padding='same',\n",
    "                                     input_shape=[1, 50, 2]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "    return model\n",
    "\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "def identity_loss(real, fake):\n",
    "    loss = tf.reduce_mean(tf.abs(real - fake))\n",
    "    return LAMBDA * 0.5 * loss\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(total, label):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        s = generator_s(total, training=True)\n",
    "        n = generator_n(total, training=True)\n",
    "        i = generator_i(total, training=True)\n",
    "        gen = (s + n + i)\n",
    "        gen = tf.reshape(gen, (1,50,2))\n",
    "        fake_t = discriminator_t(gen, training=True)\n",
    "        real_t = discriminator_t(total, training=True)\n",
    "        gen_loss = generator_loss(gen)\n",
    "        fake_d = discriminator_d(s, training=True)\n",
    "        real_d = discriminator_d(label, training=True)\n",
    "        disc_t_loss = discriminator_loss(real_t, fake_t)\n",
    "        disc_d_loss = discriminator_loss(real_d, fake_d)\n",
    "        identity_s_loss = identity_loss(label, s)\n",
    "        identity_g_loss = identity_loss(total, gen)\n",
    "        total_s_loss = 0.2*(gen_loss+identity_g_loss) + 0.8*(identity_s_loss)\n",
    "        total_n_loss = identity_g_loss + gen_loss\n",
    "        total_i_loss = identity_g_loss + gen_loss\n",
    "\n",
    "    gradients_of_s_generator = tape.gradient(total_s_loss, generator_s.trainable_variables)\n",
    "    gradients_of_i_generator = tape.gradient(total_i_loss, generator_i.trainable_variables)\n",
    "    gradients_of_n_generator = tape.gradient(total_n_loss, generator_n.trainable_variables)\n",
    "    gradients_of_discriminator_t = tape.gradient(disc_t_loss, discriminator_t.trainable_variables)\n",
    "    gradients_of_discriminator_d = tape.gradient(disc_d_loss, discriminator_d.trainable_variables)\n",
    "    generator_s_optimizer.apply_gradients(zip(gradients_of_s_generator, generator_s.trainable_variables))\n",
    "    generator_i_optimizer.apply_gradients(zip(gradients_of_i_generator, generator_i.trainable_variables))\n",
    "    generator_n_optimizer.apply_gradients(zip(gradients_of_n_generator, generator_n.trainable_variables))\n",
    "    discriminator_t_optimizer.apply_gradients(zip(gradients_of_discriminator_t, discriminator_t.trainable_variables))\n",
    "    discriminator_d_optimizer.apply_gradients(zip(gradients_of_discriminator_d, discriminator_d.trainable_variables))\n",
    "\n",
    "def shuffle_data(my_table):\n",
    "    real_y = (2*my_table.real.min())/(my_table.real.max() - my_table.real.min()) + 1\n",
    "    real_x = (my_table.real.max()) / (1 + real_y)\n",
    "    imag_y = (2*my_table.imag.min())/(my_table.imag.max() - my_table.imag.min()) + 1\n",
    "    imag_x = (my_table.imag.max()) / (1 + imag_y)\n",
    "    my_table.real = (my_table.real / real_x) - real_y\n",
    "    my_table.imag = (my_table.imag/ imag_x) - imag_y\n",
    "    train_feature = data.loc[:, ('real', 'imag')]\n",
    "    train_label = data.loc[:, ('label_real', 'label_imag')]\n",
    "    test_feature = tf.cast(train_feature, tf.float32)\n",
    "    test_label = tf.cast(train_label, tf.float32)\n",
    "    test_feature = tf.reshape(test_feature,(1000,1,50,2))\n",
    "    test_label = tf.reshape(test_label, (1000,1,50,2))\n",
    "    symbol = data.loc[:, 'label']\n",
    "    return test_feature, test_label\n",
    "\n",
    "generator_s = make_generator()\n",
    "generator_n = make_generator()\n",
    "generator_i = make_generator()\n",
    "discriminator_t = make_discriminator_model()\n",
    "discriminator_d = make_discriminator_model()\n",
    "\n",
    "\n",
    "generator_s_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "generator_n_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "generator_i_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "discriminator_d_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_t_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "LAMBDA = 10\n",
    "EPOCHS = 5\n",
    "data1 = \"my_data\"\n",
    "data1_label = \"my_labels\"\n",
    "data = dataset(data1, data1_label)\n",
    "file_directory = './result/tes2/'\n",
    "f, l = shuffle_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....._____Test Result:_____\n",
      "The generator total loss is tf.Tensor(2.9789371, shape=(), dtype=float32)\n",
      "The signal loss is  tf.Tensor(3.6672325, shape=(), dtype=float32)\n",
      "___________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    n = 0\n",
    "    for i in range(len(f)):\n",
    "        test = f[i]\n",
    "        label = l[i]\n",
    "        train_step(test, label)\n",
    "        if n % 10 == 0:\n",
    "            print('.', end='')\n",
    "            n += 1\n",
    "\n",
    "    if ((epoch + 1) % 5) == 0:\n",
    "        id = str(epoch)\n",
    "        feature = tf.reshape(f, (1000,50,2))\n",
    "        s = generator_s(feature, training=False)\n",
    "        i = generator_i(feature, training=False)\n",
    "        n = generator_n(feature, training=False)\n",
    "        gen = s + i + n\n",
    "        test = identity_loss(s, l)\n",
    "        gen_loss = identity_loss(gen, f)\n",
    "        print(\"_____Test Result:_____\")\n",
    "        print('The generator total loss is', gen_loss)\n",
    "        print('The signal loss is ', test)\n",
    "        print(\"___________________\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = tf.reshape(f, (1000,50,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1000, 50, 2), dtype=float32, numpy=\n",
       "array([[[ 0.40509632,  0.23655541],\n",
       "        [ 0.3756586 ,  0.39341068],\n",
       "        [-0.2870674 , -0.95731866],\n",
       "        ...,\n",
       "        [ 0.37447283, -0.588995  ],\n",
       "        [ 0.11752415,  0.74881446],\n",
       "        [ 0.09155288, -0.6458106 ]],\n",
       "\n",
       "       [[ 0.6310795 ,  0.17683972],\n",
       "        [ 0.20646688, -0.67077005],\n",
       "        [ 0.11570424, -0.7372731 ],\n",
       "        ...,\n",
       "        [-0.6772567 ,  0.3904876 ],\n",
       "        [-0.6811236 ,  0.39603755],\n",
       "        [ 0.6285023 ,  0.14890677]],\n",
       "\n",
       "       [[-0.7164279 ,  0.20317858],\n",
       "        [ 0.12428743, -0.52354205],\n",
       "        [-0.5589912 ,  0.38675755],\n",
       "        ...,\n",
       "        [ 0.43070465, -0.35871696],\n",
       "        [ 0.4680814 ,  0.4430951 ],\n",
       "        [ 0.59764236, -0.74916375]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.05729899, -1.2391456 ],\n",
       "        [ 0.685935  , -0.27768683],\n",
       "        [ 0.4686495 , -0.4127277 ],\n",
       "        ...,\n",
       "        [ 0.15725869, -1.1650224 ],\n",
       "        [ 0.62442416,  0.5698969 ],\n",
       "        [-0.6225406 , -0.4409476 ]],\n",
       "\n",
       "       [[-0.28865921, -0.92304575],\n",
       "        [ 0.46072716, -0.2947994 ],\n",
       "        [ 0.5542293 , -0.66252697],\n",
       "        ...,\n",
       "        [-0.56272614,  0.4700725 ],\n",
       "        [ 0.4061321 ,  0.84386086],\n",
       "        [ 0.07193783, -1.2521743 ]],\n",
       "\n",
       "       [[ 0.20898476, -0.63777673],\n",
       "        [-0.02011028, -0.70833886],\n",
       "        [-0.6241235 ,  0.31328344],\n",
       "        ...,\n",
       "        [ 0.380852  ,  0.62223566],\n",
       "        [ 0.17588386, -0.75313556],\n",
       "        [ 0.6416544 ,  0.13939597]]], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator_s(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
